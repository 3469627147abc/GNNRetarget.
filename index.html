<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta property="og:title" content="GNNRetarget: Vision-Guided Motion Retargeting Based on Graph Neural Network for Dexterous Robot"/>
  <meta property="og:url" content="https://3469627147abc.github.io/GNNRetarget./"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <title>GNNRetarget: Vision-Guided Motion Retargeting Based on Graph Neural Network for Dexterous Robot</title>
  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="icon" href="static/figures/icon2.png">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="static/js/audioPlayer.js" defer></script>
</head>

<body>

  
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://3469627147abc.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://3469627147abc.github.io/Self-AWare/">
            Self-AWare
          </a>
          <a class="navbar-item" href="https://3469627147abc.github.io/I-CTRL/">
            I-CTRL
          </a>
          <a class="navbar-item" href="https://3469627147abc.github.io/ECHO/">
            ECHO
          </a>
          <a class="navbar-item" href="https://3469627147abc.github.io/HOI4ABOT_page/">
            HOI4ABOT
          </a>
           <a class="navbar-item" href="https://3469627147abc.github.io/UNIMASKM-page/">
            UNIMASK-M
          </a>
          <a class="navbar-item" href="https://3469627147abc.github.io/icvae-page/">
            IntentionCVAE
          </a>
          <a class="navbar-item" href="https://3469627147abc.github.io/HOIGaze-page/">
            HOIGaze
          </a>
            <a class="navbar-item" href="https://3469627147abc.github.io/2CHTR-page/">
            2CHTR
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="publication-header">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <!-- <div class="columns is-centered"> -->
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">GNNRetarget: Vision-Guided Motion Retargeting Based on Graph Neural Network for Dexterous Robot</h1>
          <div class="is-size-3 publication-authors">
            RAL, 2025
          </div>
        </div>
    </div>
  </div>
</section>
<section class="publication-author-block">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">		
		
          <div class="is-size-5 publication-authors">
            <span class="author-block">Yuanchuan Lai, Qing Gao*, Xin Zhang, and Zhaojie Ju</span> 
          </div>
		
          <div class="is-size-5 publication-authors">
            <span class="author-block">Sun Yat-sen University, University of Portsmouth</span> 
          </div>
          
          <div class="column has-text-centered">
            <div class="publication-links">
  

              
              <!-- PDF Link. -->

              <!-- </span> -->
              <!-- Colab Link. -->
<!--              <span class="link-block">-->
<!--                <a href="ADD HERE THE CODE" target="_blank"-->
<!--                class="external-link button is-normal is-rounded">-->
<!--                <span class="icon">-->
<!--                  <i class="fab fa-github"></i>-->
<!--                </span>-->
<!--                <span>Code</span>-->
<!--              </a>-->
<!--             </span>-->

<!--              <span class="link-block">-->
<!--                <a href="ADD HERE REPLICATE IF NEEDED" target="_blank"-->
<!--                class="external-link button is-normal is-rounded">-->
<!--                <span class="icon">-->
<!--                  <i class="fas fa-rocket"></i>-->
<!--                </span>-->
<!--                <span>Demo</span>-->
<!--              </a>-->
<!--              </span>-->
              <!-- </span> -->
              <!-- Colab Link. -->
		    <!-- Play/Pause button and audio -->

	    </span>
	  </div>

		 <!-- Hidden Playback Controls + Disclaimer -->
	  <div id="audio-controls" style="display: none; margin-top: 15px;">
	    <div>
	      <input id="seek-bar" type="range" min="0" value="0" step="1" style="width: 100%;">
	      <span id="time-remaining">0:00</span> / <span id="duration">0:00</span>
	    </div>
	    
	    <!-- Disclaimer -->
	    <div class="disclaimer" style="margin-top: 10px;">
  <p><strong>Disclaimer:</strong> This audio was generated by AI <a href="https://notebooklm.google/" target="_blank">NotebookLM</a> and might contain false or misleading information about the paper. Please refer to the original paper for accurate details.</p>

	    </div>
	  </div>

	  <!-- Toggle Visibility Icon -->
	  <div id="toggle-visibility-icon" style="display: none; text-align: right; margin-top: 5px;">
	    <i id="visibility-icon" class="fas fa-eye" style="cursor: pointer;" onclick="toggleControls()"></i>
	  </div>
	</div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- 
<section class="hero is-small">
  <!~~ <div class="hero-body"> ~~>
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!~~ <div id="results-carousel" class="carousel results-carousel"> ~~>
      <div class="container">
      <div class="item">
      <div class="column is-centered has-text-centered">
        <img src="static/figures/teaser.png" alt="UNIMASKM"/>
      </div>

    </div>
  </div>
 <!~~  </div> ~~>
  </div>
  </div>
 <!~~  </div> ~~>
</section>
 -->

  <section class="hero is-small">

    
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">

        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
         Motion retargeting is pivotal in executing complex
tasks by transferring human motion to robotic systems with
precision. However, existing motion retargeting approaches
often require elaborate human motion capture setups and
fail to account for the scale discrepancies between hands and
bodies, resulting in suboptimal accuracy. Additionally, their
retargeting methods face challenges such as high computational
complexity, susceptibility to local minima, and inaccuracies in
mapping unseen motions. Moreover, these methods are typically
tailored to specific robot platforms, lacking generalization
capabilities. In this paper, we propose a novel vision-guided
motion retargeting framework and approach to address these
limitations. Our framework employs a dual-stream architecture, processing hands and bodies separately to effectively
mitigate scale discrepancies, thereby enhancing precision. The
proposed retargeting method integrates a graph encoder network to generate meaningful initial embeddings, subsequently
optimized in the latent space. This strategy significantly reduces
complexity while circumventing local minima issues. Crucially,
by graphically modeling human poses and robot files, our
method eliminates the need for paired datasets, enabling broad
applicability across diverse robots. Experiments successfully
replicate human motions and validate the feasibility and accuracy of human-robot motion retargeting in both simulated
environments on RMC-DA, YuMi, and Unitree H1, as well as
the real-world RMC-DA, underscoring the practical value of
our vision-guided motion retargeting framework.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

      <section class="hero is-small">
    <div class="hero-body">
      <div class="container">

<div class="content has-text-centered">
<video id="replay-video" controls muted preload playsinline width="95%">
<source src="static/figures/videos/Motion Retaregting.mp4" type="video/mp4">
</video>
</div>   
	      

      </div>
    </div>
  </section>






      <section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Vision-Guided Robot Motion Retargeting</h2>
          <p>
            We estimate the human pose from RGB images using an off-the-shelf pose estimator, and feed our proposed GNNRetarget model with the predicted human pose. Thanks to its lightweight design and high retargeting accuracy, our model can directly control the real RMC-DA robot.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>
	  
      <section class="hero is-small">
    <div class="hero-body">
      <div class="container">

<div class="content has-text-centered">
<video id="replay-video1" controls muted preload playsinline width="95%">
<source src="static/figures/videos/Retargting Generalizability.mp4" type="video/mp4">
</video>
</div>    

      </div>
    </div>
  </section>
	  
      <section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Retargeting To Different Robots</h2>
          <p>
          We take multiple human motion sequences and map them onto different robots through our motion retargeting algorithm. This algorithm allows us to transfer the captured human movements onto robots with varying kinematic structures and degrees of freedom.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

      <section class="hero is-small">
    <div class="hero-body">
      <div class="container">

<div class="content has-text-centered">
<video id="replay-video1" controls muted preload playsinline width="95%">
<source src="static/figures/videos/Retargeting Method Compare.mp4" type="video/mp4">
</video>
</div>    

      </div>
    </div>
  </section>


	        <section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparison Of Different Retargeting Methods</h2>
          <p>
          Here is a comparison video of different motion retargeting methods in our paper, comparing the NLO method with our approach.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


	        <section class="hero is-small">
    <div class="hero-body">
      <div class="container">

<div class="content has-text-centered">
<video id="replay-video1" controls muted preload playsinline width="95%">
<source src="static/figures/videos/Hand Retargeting Comparison.mp4" type="video/mp4">
</video>
</div>    

      </div>
    </div>
  </section>

	        <section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Hand Retargeting Comparison</h2>
          <p>
         Here are various demonstrations of hand motion retargeting dexterity.This demonstration further showcases the dexterity of hand motion retargeting.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

	  
	  
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">

      <div class="column is-centered has-text-centered">
        <img src="static/figures/Figure2.png" alt="Motivation of our model" width="90%">
      </div>
  </div>
</div>
</div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Framework For Motion Retargeting</h2>
          <p>
         The framework consists of two parts: a visual teleoperation pipeline and a graph neural network-based motion retargeting module called GNNRetarget. The visual teleoperation part captures the operator's motion from an RGB camera and extracts hand and body motion data. The GNNRetarget module takes these motion data as input to encoders, performs latent optimization to map them to robot hand and body motions, which are then executed on the physical robot after optimization.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


    
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
  author    = {Yuanchuan Lai, Qing Gao, Xin Zhang, and Zhaojie Ju},
  title     = {GNNRetarget: Vision-Guided Dexterous Robot Teleoperation via Graph Neural Network-Based Motion Retargeting},
  journal   = {RAL},
  year      = {2024},
</code></pre>
    </div>
</section>




<footer class="footer">
 <!--  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
      href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf">
      <i class="fas fa-file-pdf"></i>
    </a>
    <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
      <i class="fab fa-github"></i>
    </a>
  </div> -->
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is licensed under a <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <p>
        Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
        href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
      </p>
    </div>
  </div>
</div>
</div>
</footer>

  </body>
  </html>
